{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e8879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2022 Diabwellness.ai, Inc.\n",
    "# All rights reserved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8926d82",
   "metadata": {},
   "source": [
    "# Risk Profiling\n",
    "\n",
    "Notebook to cluster patients using VaDER network and profile them with potential risk factors. This type of clustering is based on the longitudinal/time-series data of the patients unlike k-means which is a crossectional clustering.\n",
    "\n",
    "We use the following tables for this analysis: \n",
    "- `measurement_details`: contains the various measurements corresponding to every appointments\n",
    "- `patient_details`: contains the age, gender and diabetic duration of the patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec8dd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from bokeh import core, io, palettes, models, layouts\n",
    "from bokeh.plotting import output_file, figure\n",
    "from bokeh.models import BoxAnnotation, LinearAxis, Range1d\n",
    "from bokeh.io import show, output_notebook\n",
    "\n",
    "io.reset_output()\n",
    "io.output_notebook()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.run_functions_eagerly(False)\n",
    "# from diabwellness.VaDER.VaDER.vader import VADER\n",
    "from VaDER.vader import VADER\n",
    "\n",
    "# import VaDER\n",
    "\n",
    "from diabwellness.utils.data_utils import (\n",
    "    preprocess_measurements,\n",
    "    measurement_aggregator,\n",
    "    combine_patient_details,\n",
    "    map_complications,\n",
    "    map_patient_types,\n",
    ")\n",
    "from diabwellness.utils.plot_utils import (\n",
    "    create_metrics_plot,\n",
    "    display_factorial_planes,\n",
    "    display_parallel_coordinates_centroids,\n",
    ")\n",
    "\n",
    "# Change jupyter notebook to full width for extra visualization space\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 50000)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d97fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the patient details:\n",
    "pat_df = pd.read_csv(\n",
    "    \"../database/patient_details.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    index_col=False,\n",
    "    parse_dates=[\"created_time\"],\n",
    ")\n",
    "pat_df.info()\n",
    "\n",
    "colnames = [\n",
    "    \"ID\",\n",
    "    \"APPOINT_ID\",\n",
    "    \"BMI\",\n",
    "    \"BP\",\n",
    "    \"COMPLAINTS\",\n",
    "    \"CREATED_BY\",\n",
    "    \"CREATED_DATE\",\n",
    "    \"DIAGNOSIS\",\n",
    "    \"HEIGHT\",\n",
    "    \"LOCATION_ID\",\n",
    "    \"NFID\",\n",
    "    \"STATUS\",\n",
    "    \"TEMPERATURE\",\n",
    "    \"UPDATED_BY\",\n",
    "    \"UPDATED_DATE\",\n",
    "    \"WC\",\n",
    "    \"WEIGHT\",\n",
    "    \"PATIENT_TYPE\",\n",
    "    \"A1C\",\n",
    "    \"DIA_BP\",\n",
    "    \"DURATION_TT\",\n",
    "    \"FS\",\n",
    "    \"NOTES\",\n",
    "    \"PP\",\n",
    "    \"PULSE\",\n",
    "    \"REVIEW_DATYS\",\n",
    "    \"ADMISSION_REQUIRED\",\n",
    "    \"REVIEW_DATE\",\n",
    "    \"LAB_FOR_NEXT_VISIT\",\n",
    "]\n",
    "meas_df = pd.read_csv(\n",
    "    \"../database/measurement_details.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    names=colnames,\n",
    "    header=None,\n",
    "    parse_dates=[\"CREATED_DATE\", \"UPDATED_DATE\"],\n",
    ")\n",
    "meas_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe6ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the measurement details:\n",
    "meas_df = preprocess_measurements(meas_df)\n",
    "# extract the diabetic complications and patient types:\n",
    "meas_df = map_complications(meas_df)\n",
    "meas_df = map_patient_types(meas_df)\n",
    "meas_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4511c786",
   "metadata": {},
   "outputs": [],
   "source": [
    "meas_filt_df = (\n",
    "    meas_df.groupby([\"NFID\"]).apply(measurement_aggregator).dropna(subset=[\"A1C\"])\n",
    ")\n",
    "meas_filt_df = combine_patient_details(meas_filt_df, pat_df)\n",
    "\n",
    "meas_filt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdd5d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "meas_filt_df[\"COMPLICATIONS\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4dd08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "meas_filt_df.loc[meas_filt_df[\"PATIENT_TYPE\"] == {\"NON-DM\", \"DM\"}].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf94dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "meas_filt_df[\"PATIENT_TYPE\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfc481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "meas_filt_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15858adb",
   "metadata": {},
   "source": [
    "## Visualize the Patient measurements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a021e3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize A1c, FS, PP and other characteristics of a patient over time:\n",
    "select_cols = [\n",
    "    \"A1C\",\n",
    "    \"FS\",\n",
    "    \"PP\",\n",
    "    \"CREATED_DATE\",\n",
    "    \"DURATION\",\n",
    "    \"PATIENT_TYPE\",\n",
    "    \"AGE\",\n",
    "    \"GENDER\",\n",
    "]\n",
    "explode_cols = [\"A1C\", \"FS\", \"PP\", \"CREATED_DATE\"]\n",
    "a1c_cond = meas_filt_df[\"A1C_counts\"] >= 2\n",
    "\n",
    "plot_list = []\n",
    "# print(input_df.iloc[2].to_frame().T.explode(cols).set_index('CREATED_DATE'))\n",
    "\n",
    "for i in range(10, 15):\n",
    "    input_df = (\n",
    "        meas_filt_df.loc[a1c_cond][select_cols]\n",
    "        .reset_index()\n",
    "        .iloc[i]\n",
    "        .to_frame()\n",
    "        .T.explode(explode_cols)\n",
    "        .set_index(\"CREATED_DATE\")\n",
    "    )\n",
    "    plot = create_metrics_plot(\n",
    "        input_df=input_df,\n",
    "        varea_plots={\n",
    "            \"PP\": [\"wheat\", \"sugar\", 0.0],\n",
    "            \"FS\": [\"chocolate\", \"sugar\", 0.0],\n",
    "        },\n",
    "        line_plots={\"A1C\": \"black\"},\n",
    "        extra_y_ranges={\"sugar\": Range1d(start=0, end=500)},\n",
    "        title=f\"Measurements for NFID {input_df['NFID'][0]}\\\n",
    "            diabetic duration (in days): {input_df['DURATION'][0].astype('int')}\\\n",
    "            patient type: {input_df['PATIENT_TYPE'][0]}\\\n",
    "            age: {input_df['AGE'][0].astype('int')}\\\n",
    "            gender: {input_df['GENDER'][0]}\",\n",
    "        stacked_area_plots=False,\n",
    "    )\n",
    "    plot.legend.orientation = \"horizontal\"\n",
    "    plot.y_range = Range1d(start=0, end=15)\n",
    "    plot_list.append(plot)\n",
    "\n",
    "\n",
    "# Link the x-axis and y-axis\n",
    "xr = plot_list[0].x_range\n",
    "yr = plot_list[0].y_range\n",
    "for pl in plot_list:\n",
    "    pl.x_range = xr\n",
    "    pl.y_range = yr\n",
    "combined_plot = layouts.column(plot_list)\n",
    "show(combined_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fe2cb1",
   "metadata": {},
   "source": [
    "## Train with VaDER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ff9c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1c_cond = meas_filt_df[\"A1C_counts\"] == 3\n",
    "meas_filt_df.loc[a1c_cond].shape\n",
    "meas_filt_df[\"A1C_counts\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7974f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1c_cond = meas_filt_df[\"A1C_counts\"] >= 2\n",
    "X_train = np.array(meas_filt_df.loc[a1c_cond][\"A1C_first_two_values\"].tolist())\n",
    "# X_train shape should be (n_patients, n_timesteps, n_variables)\n",
    "# Note: All X_train[i,j] for which W_train[i,j] == 0 are treated as missing (i.e. their specific value is ignored)\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4714db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd8618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize (better for fitting)\n",
    "# for i in np.arange(X_train.shape[2]):\n",
    "#     print(i)\n",
    "#     X_train[:,:,i] = (X_train[:,:,i] - np.mean(X_train[:,:,i])) / np.std(X_train[:,:,i])\n",
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc987b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: y_train is used purely for monitoring performance when a ground truth clustering is available.\n",
    "# It can be omitted if no ground truth is available.\n",
    "from pathlib import Path\n",
    "Path(\"./vader_checkpoints\").mkdir(parents=True, exist_ok=True)\n",
    "save_path = os.path.join(\"./vader_checkpoints\", \"vader_a1c.ckpt\")\n",
    "\n",
    "vader = VADER(\n",
    "    X_train=X_train,\n",
    "    W_train=None,\n",
    "    y_train=None,\n",
    "    save_path=save_path,\n",
    "    n_hidden=[12, 2],\n",
    "    k=4,\n",
    "    learning_rate=1e-3,\n",
    "    output_activation=None,\n",
    "    recurrent=True,\n",
    "    cell_type=\"LSTM\",\n",
    "    batch_size=64,\n",
    ")\n",
    "# pre-train without latent loss\n",
    "start = time.time()\n",
    "vader.pre_fit(n_epoch=50, verbose=True)\n",
    "# train with latent loss\n",
    "vader.fit(n_epoch=50, verbose=True)\n",
    "end = time.time()\n",
    "print(\"Elapsed: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d534278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the clusters\n",
    "c = vader.cluster(X_train)\n",
    "# get the re-constructions\n",
    "p = vader.predict(X_train)\n",
    "# compute the loss given the network\n",
    "l = vader.get_loss(X_train)\n",
    "\n",
    "print(\"clusters: \", c)\n",
    "# print(\"reconstructions: \", p)\n",
    "# print(\"losses: \", l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159cb750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the cluster index to the original dataframe:\n",
    "meas_filt_df[\"CLUSTER_INDEX\"] = np.nan\n",
    "a1c_cond = meas_filt_df[\"A1C_counts\"] >= 2\n",
    "\n",
    "cluster_series = pd.Series(c, index=meas_filt_df.loc[a1c_cond].index, dtype=\"int\")\n",
    "meas_filt_df.loc[a1c_cond, \"CLUSTER_INDEX\"] = meas_filt_df.loc[\n",
    "    a1c_cond, \"CLUSTER_INDEX\"\n",
    "].fillna(cluster_series)\n",
    "\n",
    "meas_filt_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8250b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_cols = [\n",
    "    \"HEIGHT\",\n",
    "    \"WEIGHT\",\n",
    "    \"BMI\",\n",
    "    \"BP\",\n",
    "    \"DIA_BP\",\n",
    "    \"PULSE\",\n",
    "    \"A1C_first_two_values\",\n",
    "    \"FS\",\n",
    "    \"PP\",\n",
    "    \"DURATION\",\n",
    "    \"PATIENT_TYPE\",\n",
    "    \"AGE\",\n",
    "    \"GENDER\",\n",
    "    \"CLUSTER_INDEX\",\n",
    "]\n",
    "\n",
    "\n",
    "def process_cluster_df(cluster_df):\n",
    "    cluster_df[\"A1C_first\"] = cluster_df[\"A1C_first_two_values\"].apply(lambda x: x[0])\n",
    "    cluster_df[\"A1C_second\"] = cluster_df[\"A1C_first_two_values\"].apply(lambda x: x[1])\n",
    "    cluster_df[\"FS\"] = cluster_df[\"FS\"].apply(lambda x: pd.Series(x).dropna().mean())\n",
    "    cluster_df[\"PP\"] = cluster_df[\"PP\"].apply(lambda x: pd.Series(x).dropna().mean())\n",
    "    #     cluster_df['PATIENT_TYPE'] = cluster_df['PATIENT_TYPE'].map('tuple').astype('category').cat.codes\n",
    "    #     cluster_df['GENDER'] = cluster_df['GENDER'].astype('category').cat.codes\n",
    "\n",
    "    cat_cols = [\"PATIENT_TYPE\", \"GENDER\", \"A1C_first_two_values\"]\n",
    "    df = cluster_df.drop(columns=cat_cols).dropna().copy()\n",
    "    df_norm = (df - df.mean()) / df.std()\n",
    "    X_scaled = df_norm.values\n",
    "    return X_scaled\n",
    "\n",
    "\n",
    "X_scaled = process_cluster_df(meas_filt_df.loc[a1c_cond, select_cols])\n",
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c81f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X_scaled)\n",
    "# pca.components_  # we only take the first two features.\n",
    "\n",
    "# Transfor the scaled data to the new PCA space\n",
    "X_reduced = pca.transform(X_scaled)\n",
    "\n",
    "# centres_reduced = pca.transform(kmeans.cluster_centers_)\n",
    "\n",
    "# fig = plt.figure()\n",
    "\n",
    "display_factorial_planes(\n",
    "    X_reduced, 2, pca, [(0, 1)], illustrative_var=X_scaled[:, 10], alpha=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f46b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "meas_filt_df[\"CLUSTER_INDEX\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7337e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "meas_filt_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a372153",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATIENT_TYPES = {\n",
    "    (\"DM\",): \"DM\",\n",
    "    (\"NON-DM\",): \"NON-DM\",\n",
    "    (\"NON-MS\",): \"NON-MS\",\n",
    "    (\n",
    "        \"THY\",\n",
    "        \"Hypothyroid\",\n",
    "        \"Hypothyroidism\",\n",
    "        \"Thyroid\",\n",
    "    ): \"THY\",\n",
    "    (\"IDDM\",): \"IDDM\",\n",
    "    (\"GDM\",): \"GDM\",\n",
    "    (\n",
    "        \"InflDisorder\",\n",
    "        \"Inf\",\n",
    "    ): \"InflDisorder\",\n",
    "    (\n",
    "        \"HT\",\n",
    "        \"HTN\",\n",
    "    ): \"HT\",\n",
    "    #     (\"Allergy\",): \"Allergy\",\n",
    "    #     (\"Anemia\",): \"Anemia\",\n",
    "    #     (\"Cancer\",): \"Cancer\",\n",
    "    (\n",
    "        \"Hlip\",\n",
    "        \"Hyperlipidemia\",\n",
    "    ): \"Hlip\",\n",
    "}\n",
    "\n",
    "COMPLICATION_TYPES = {\n",
    "    (\n",
    "        \"CAD\",\n",
    "        \"Angina\",\n",
    "        \"Unstable angina\",\n",
    "        \"ASMI\",\n",
    "        \"AWMI\",\n",
    "        \"IWMI\",\n",
    "        \"PTCA\",\n",
    "        \"CABG\",\n",
    "        \"heart attack\",\n",
    "        \"infarct\",\n",
    "        \"ischemia\",\n",
    "    ): \"CAD\",\n",
    "    (\n",
    "        \"Stroke\",\n",
    "        \"hemiplegia\",\n",
    "        \"hemiparesis\",\n",
    "        \"cerebral infarct\",\n",
    "        \"VBI\",\n",
    "        \"vertebrobasilar infarct\",\n",
    "        \"pontine infarct\",\n",
    "        \"PICA\",\n",
    "        \"AICA\",\n",
    "        \"cerebellar infarct\",\n",
    "    ): \"CVD\",\n",
    "    (\n",
    "        \"Intermittent claudication\",\n",
    "        \"pain legs on walking\",\n",
    "        \"pain calf muscles on walking\",\n",
    "    ): \"PVD\",\n",
    "    (\n",
    "        \"PN\",\n",
    "        \"painful peripheral Neuropathy\",\n",
    "        \"numbness\",\n",
    "        \"tingling\",\n",
    "        \"burning feet\",\n",
    "        \"numb feet\",\n",
    "        \"numb limbs\",\n",
    "        \"DPN\",\n",
    "    ): \"DPN\",\n",
    "    (\n",
    "        \"DKD\",\n",
    "        \"CKD\",\n",
    "        \"Microalbuminuria\",\n",
    "        \"Macroalbuminuria\",\n",
    "        \"DN\",\n",
    "        \"Nephropathy\",\n",
    "        \"diabetic nephropathy\",\n",
    "    ): \"DN\",\n",
    "    (\n",
    "        \"DR\",\n",
    "        \"PDR\",\n",
    "        \"NPDR\",\n",
    "        \"Diabetic retinopathy\",\n",
    "    ): \"DR\",\n",
    "    (\n",
    "        \"Foot ulcer\",\n",
    "        \"bleb\",\n",
    "        \"wound\",\n",
    "        \"amputee\",\n",
    "        \"fissure\",\n",
    "        \"callus\",\n",
    "        \"corn\",\n",
    "        \"DFU\",\n",
    "        \"Nonhealing ulcer\",\n",
    "    ): \"DFU\",\n",
    "}\n",
    "\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "\n",
    "def cluster_aggregator(group):\n",
    "    output_dict = {}\n",
    "\n",
    "    def extract_set_fractions(series, types_dict):\n",
    "        counts_dict = Counter(\n",
    "            {given_type: 0 for given_type in set(types_dict.values())}\n",
    "        )\n",
    "        cluster_list = series.dropna().apply(lambda x: list(x)).tolist()\n",
    "        merged_list = list(itertools.chain(*cluster_list))\n",
    "        counts_dict.update(merged_list)\n",
    "        return {\n",
    "            f\"{series.name}_{k}\": f\"{v}/{series.size}\" for k, v in counts_dict.items()\n",
    "        }\n",
    "\n",
    "    # columns where average value should be taken\n",
    "    mean_cols = [\"HEIGHT\", \"WEIGHT\", \"BMI\", \"BP\", \"DIA_BP\", \"PULSE\", \"DURATION\", \"AGE\"]\n",
    "    output_dict = {col: group[col].mean() for col in mean_cols}\n",
    "\n",
    "    # count of new T2DM patients in each cluster\n",
    "    output_dict[\"NEW_T2DM\"] = group[\"NEW_T2DM\"].sum()\n",
    "\n",
    "    # first, second and last values of A1C values are considered\n",
    "    output_dict[\"A1C_first\"] = (\n",
    "        group[\"A1C_first_two_values\"].apply(lambda x: x[0]).mean()\n",
    "    )\n",
    "    output_dict[\"A1C_second\"] = (\n",
    "        group[\"A1C_first_two_values\"].apply(lambda x: x[1]).mean()\n",
    "    )\n",
    "    output_dict[\"A1C_last\"] = (\n",
    "        group[\"A1C\"].apply(lambda x: pd.Series(x).dropna().iloc[-1]).mean()\n",
    "    )\n",
    "\n",
    "    # get the size of patients in each cluster:\n",
    "    output_dict[\"CLUSER_SIZE\"] = group.shape[0]\n",
    "\n",
    "    # first and last values of FF and PP  are considered\n",
    "    #     print(group['FS'])\n",
    "    #     output_dict['FS_first'] = group['FS'].apply(lambda x: pd.Series(x).dropna().iloc[0]).mean()\n",
    "    #     output_dict['FS_last'] = group['FS'].apply(lambda x: pd.Series(x).dropna().iloc[-1]).mean()\n",
    "    #     output_dict['PP_first'] = group['PP'].apply(lambda x: pd.Series(x).dropna().iloc[0]).mean()\n",
    "    #     output_dict['PP_last'] = group['PP'].apply(lambda x: pd.Series(x).dropna().iloc[-1]).mean()\n",
    "\n",
    "    # combine COMPLICATIONS, PATIENT_TYPE into a single set and then to proportions:\n",
    "    output_dict.update(\n",
    "        extract_set_fractions(group[\"COMPLICATIONS\"], COMPLICATION_TYPES)\n",
    "    )\n",
    "    output_dict.update(extract_set_fractions(group[\"PATIENT_TYPE\"], PATIENT_TYPES))\n",
    "\n",
    "    return pd.Series(output_dict)\n",
    "\n",
    "\n",
    "# check for Waist circumference (WC) from Patient details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca830254",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = meas_filt_df.groupby([\"CLUSTER_INDEX\"]).apply(cluster_aggregator)\n",
    "cluster_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7891ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import parallel_coordinates\n",
    "import seaborn as sns\n",
    "\n",
    "palette = sns.color_palette(\"bright\", 10)\n",
    "\n",
    "# ax = display_parallel_coordinates_centroids(cluster_df, 10)\n",
    "\n",
    "cluster_df = (cluster_df - cluster_df.mean()) / cluster_df.std()\n",
    "cluster_df[\"cluster\"] = cluster_df.index\n",
    "\n",
    "# Create the plot\n",
    "fig = plt.figure(figsize=(30, 10))\n",
    "fig.suptitle(\"Parallel Coordinates plot for the Centroids\")\n",
    "fig.subplots_adjust(top=0.9, wspace=0)\n",
    "\n",
    "# Draw the chart\n",
    "ax = parallel_coordinates(cluster_df, \"cluster\", color=palette)\n",
    "plt.xticks(rotation=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62b87f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTES:\n",
    "# variables for vader: A1c, age, duration, BMI, FS, PP, BP, DIA_BP, PULSE\n",
    "# report ratios based on cluster size as well as total patients in each compl/pat_type\n",
    "\n",
    "# data based on time periods: (2017' March to 2021)? choose n_timesteps average? year end timesteps?\n",
    "# arbitrary value of timesteps?\n",
    "\n",
    "# P1: t1,t2, t3 -> t1, t2, t3, t4, t5\n",
    "#     A, 0, A\n",
    "#     F, F, F\n",
    "#     0, 0 ,0\n",
    "\n",
    "# P2: t1, t2, t3, .. t10 -> t1, t2, t3, t4, t5\n",
    "#     0, A, 0,    .. A\n",
    "#     0, F, 0,   ..  0\n",
    "#     P, P, P,  . .  P\n",
    "\n",
    "# P3: t1\n",
    "#     0\n",
    "#     F\n",
    "#     P\n",
    "\n",
    "\n",
    "# X_train = n_patients x n_timesteps x n_variables\n",
    "#         = N x 5 x 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
